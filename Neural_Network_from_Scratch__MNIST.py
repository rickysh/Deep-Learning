###################################
###### Ricky Shama ######
###### Lena Pekarsky ####
###################################


from keras.datasets import mnist
from sklearn.model_selection import train_test_split
import numpy as np
import sys
import time


###########################################
###### Forward propagation functions ######
###########################################


#  1.a  parameters{'W1'…'WL', 'b1'…'bL'}
def initialize_parameters(layer_dims):  # layer_dims = [n, 20, 7, 5, num_of_classes]
    """ input: an array of the dimensions of each layer in the network (layer 0 is the size of the flattened input,
        layer L is the output softmax)
        output: a dictionary containing the initialized W and b parameters of each layer (W1…WL, b1…bL).
        Hint: Use the randn and zeros functions of numpy to initialize W and b, respectively
    """
    L = len(layer_dims) - 1
    parameters = {}
    for l in range(1, L + 1):
        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * np.sqrt(2 / layer_dims[l - 1])
        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))
    parameters['W' + str(L)] = np.random.randn(layer_dims[L], layer_dims[L - 1]) * np.sqrt(2 / (layer_dims[L - 1] +
                                                                                                layer_dims[L]))
    return parameters


#  1.b  (Z, linear_cache{'A', 'W', 'b'}) ** Layer level
def linear_forward(A, W, b):
    """ Description: Implement the linear part of a layer's forward propagation.
        input:
        A – the activations of the previous layer
        W – the weight matrix of the current layer (of shape [size of current layer, size of previous layer])
        B – the bias vector of the current layer (of shape [size of current layer, 1])
        Output:
        Z – the linear component of the activation function (i.e., the value before applying the non-linear function)
        linear_cache – a dictionary containing A, W, b (stored for making the backpropagation easier to compute)
    """
    Z = np.dot(W, A) + b
    linear_cache = {'A': A, 'W': W, 'b': b}
    return Z, linear_cache


#  1.c  (A(softmax activation), activation_cache(Z)) ** Layer level
def softmax(Z):
    """ Input:
        Z – the linear component of the activation function
        Output:
        A – the activations of the layer
        activation_cache – returns Z, which will be useful for the backpropagation
        note:
        Softmax can be thought of as a sigmoid for multi-class problems.
    """
    A = np.exp(Z) / np.sum(np.exp(Z), axis=0)
    activation_cache = Z
    return A, activation_cache


#  1.d  (A(ReLU activation), activation_cache(Z)) ** Layer level
def relu(Z):
    """ Input:
        Z – the linear component of the activation function
        Output:
        A – the activations of the layer
        activation_cache – returns Z, which will be useful for the backpropagation
    """
    A = np.maximum(0, Z)
    activation_cache = Z
    return A, activation_cache


#  1.e  (A, cache{"linear_cache": {'A_prev', 'W', 'b'}, "activation_cache": Z}) ** Layer level
def linear_activation_forward(A_prev, W, B, activation):
    """ Description:
        Implement the forward propagation for the LINEAR->ACTIVATION layer
        Input:
        A_prev – activations of the previous layer
        W – the weights matrix of the current layer
        B – the bias vector of the current layer
        Activation – the activation function to be used (a string, either “softmax” or “relu”)
        Output:
        A – the activations of the current layer
        cache – a joint dictionary containing both linear_cache and activation_cache
    """
    Z, linear_cache__A_prev_W_B = linear_forward(A_prev, W, B)  # Z, {'A_prev', 'W', 'b'}
    A, activation_cache__Z = relu(Z) if activation == "relu" else softmax(Z)  # A, Z
    cache = {"linear_cache": linear_cache__A_prev_W_B, "activation_cache": activation_cache__Z}
    #  {"linear_cache": {'A_prev', 'W', 'b'}, "activation_cache": Z}
    return A, cache


#  1.f  (AL, caches[cache])
def L_model_forward(X, parameters, use_batchnorm, use_dropout, keep_prob):
    """ Description:
        Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation
        Input:
        X – the data, numpy array of shape (input size, number of examples)
        parameters – the initialized W and b parameters of each layer
        use_batchnorm - a boolean flag used to determine whether to apply batchnorm after the activation
        Output:
        AL – the last post-activation value
        caches – a list of all the cache objects generated by the linear_forward function
    """
    #  Auxiliary function
    #  (A, caches[cache])
    def compute_forward(l, parameters, A_prev, activation, caches, use_batchnorm, use_dropout, keep_prob):
        W = parameters['W' + str(l)]
        b = parameters['b' + str(l)]
        A, cache = linear_activation_forward(A_prev, W, b, activation)
        if use_dropout:
            M = np.random.rand(A.shape[0], A.shape[1]) < keep_prob[l]  # M = Mask matrix
            A *= M
        if use_batchnorm:
            A = apply_batchnorm(A)
        caches.append(cache)
        return A, caches


    caches = []
    L = len(parameters) // 2
    A_prev = X
    for l in range(1, L):
        A, caches = compute_forward(l, parameters, A_prev, "relu", caches, use_batchnorm, use_dropout, keep_prob)
        A_prev = A
    AL, caches = compute_forward(L, parameters, A_prev, "softmax", caches, False, False, [])
    return AL, caches


#  1.g  cost
def compute_cost(AL, Y):
    """ Description:
        Implement the cost function defined by equation. The requested cost function is categorical cross-entropy loss.
        Input:
        AL – probability vector corresponding to your label predictions, shape (num_of_classes, number of examples)
        Y – the labels vector (i.e. the ground truth)
        Output:
        cost – the cross-entropy cost
    """
    c = AL.shape[0]  # no. of classes (=10, no. of digits), rows, Y rows for each sample (yi, "one-hot")
    m = AL.shape[1]  # no. of samples, columns, Y columns
    cost_per_sample_matrix = np.ma.log(np.exp(AL) / np.sum(np.exp(AL), axis=0)) * Y
    cost_per_sample_matrix = cost_per_sample_matrix.filled(0)
    cost = - np.sum(cost_per_sample_matrix) / m
    return cost


#  1.h
def apply_batchnorm(A):
    """ Description:
        performs batchnorm on the received activation values of a given layer.
        Input:
        A - the activation values of a given layer
        output:
        NA - the normalized activation values, based on the formula learned in class
    """
    n = A.shape[0]
    m = A.shape[1]
    mu = np.mean(A, axis=0)
    variance = np.var(A, axis=0)
    A_mu = np.tile(mu, (n, 1))
    A_var = np.tile(variance, (n, 1))
    epsilon = sys.float_info.epsilon
    A_e = np.ones((n, m))*epsilon
    NA = (A - A_mu) / (np.sqrt(A_var + A_e))
    return NA


############################################
###### Backward propagation functions ######
############################################


#  2.a  (dA_prev(l-1), dW(l), db(l)) ** Layer level
def Linear_backward(dZ, cache):
    """ description:
        Implements the linear part of the backward propagation process for a single layer
        Input:
        dZ – the gradient of the cost with respect to the linear output of the current layer (layer l)
        cache – tuple of values (A_prev, W, b) coming from the forward propagation in the current layer
        Output:
        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
        dW -- Gradient of the cost with respect to W (current layer l), same shape as W
        db -- Gradient of the cost with respect to b (current layer l), same shape as b
    """
    m = dZ.shape[1]  # columns
    A_prev, W, b = cache['A'], cache['W'], cache['b']
    dA_prev = np.dot(W.T, dZ)
    dW = np.dot(dZ, A_prev.T) / m
    db = np.sum(dZ, axis=1, keepdims=True) / m
    return dA_prev, dW, db


#  2.b  (dA_prev(l-1), dW(l), db(l)) ** Layer level
def linear_activation_backward(dA, cache, activation):
    """
        Description:
        Implements the backward propagation for the LINEAR->ACTIVATION layer.
        The function first computes dZ and then applies the linear_backward function.
        Some comments:
        * The derivative of the softmax function is: pi-yi, where pi is the softmax-adjusted probability of the class
          and y_i is the “ground truth” (i.e. 1 for the real class, 0 for all others)
        * You should use the activations cache created earlier for the calculation of the activation derivative
          and the linear cache should be fed to the linear_backward function
        Input:
        dA – post activation gradient of the current layer
        cache – contains both the linear cache and the activations cache
        Output:
        dA_prev – Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
        dW – Gradient of the cost with respect to W (current layer l), same shape as W
        db – Gradient of the cost with respect to b (current layer l), same shape as b
    """
    activation_cache = cache["activation_cache"]  # Z
    dZ = relu_backward(dA, activation_cache) if activation == "relu" \
        else softmax_backward(dA, activation_cache)
    dA_prev, dW, db = Linear_backward(dZ, cache["linear_cache"])
    return dA_prev, dW, db


#  2.c  dZ(ReLU) ** Layer level
def relu_backward(dA, activation_cache):
    """
        Description:
        Implements backward propagation for a ReLU unit
        Input:
        dA – the post-activation gradient
        activation_cache – contains Z (stored during the forward propagation)
        Output:
        dZ – gradient of the cost with respect to Z
    """
    Z = activation_cache
    dA_dZ = Z
    dA_dZ[dA_dZ <= 0] = 0
    dA_dZ[dA_dZ > 0] = 1
    dZ = dA * dA_dZ
    return dZ


#  2.d  dZ(softmax) ** Layer level
def softmax_backward(dA, activation_cache):
    """
        Description:
        Implements backward propagation for a softmax unit
        Input:
        dA – the post-activation gradient
        activation_cache – contains Z (stored during the forward propagation)
        Output:
        dZ – gradient of the cost with respect to Z
    """
    Z = activation_cache
    A, _ = softmax(Z)
    n = A.shape[0]
    m = A.shape[1]
    dA_dZ = A * (np.ones((n, m)) - A)
    dZ = dA * dA_dZ
    return dZ


#  2.e  grads{'dA1'…'dAL', 'dW1'…'dWL', 'db1'…'dbL'}
def L_model_backward(AL, Y, caches):
    """
        Description:
        Implement the backward propagation process for the entire network.
        Some comments:
        the backpropagation for the softmax function should be done only once as only the output layers
        use it and the RELU should be done iteratively over all the remaining layers of the network.
        Input:
        AL - the probabilities vector, the output of the forward propagation (L_model_forward)
        Y - the true labels vector (the "ground truth" - true classifications)
        Caches - list of caches containing for each layer: a) the linear cache; b) the activation cache
        Output:
        Grads - a dictionary with the gradients
                 grads["dA" + str(l)] = ...
                 grads["dW" + str(l)] = ...
                 grads["db" + str(l)] = ...
    """
    L = len(caches)
    grads = {}
    dZ_L = AL - Y

    grads["dA" + str(L - 1)], grads["dW" + str(L)], grads["db" + str(L)] = \
        Linear_backward(dZ_L, caches[L - 1]["linear_cache"])

    for l in range(L - 1, 0, -1):
        grads["dA" + str(l - 1)], grads["dW" + str(l)], grads["db" + str(l)] = \
            linear_activation_backward(grads["dA" + str(l)], caches[l - 1], "relu")

    return grads


#  2.f  parameters{'W1'…'WL', 'b1'…'bL'}
def Update_parameters(parameters, grads, learning_rate):
    """
        Description:
        Updates parameters using gradient descent
        Input:
        parameters – a python dictionary containing the DNN architecture’s parameters
        grads – a python dictionary containing the gradients (generated by L_model_backward)
        learning_rate – the learning rate used to update the parameters (the “alpha”)
        Output:
        parameters – the updated values of the parameters object provided as input
    """
    alpha = learning_rate
    L = len(parameters) // 2
    for l in range(1, L + 1):
        parameters['W' + str(l)] -= alpha*grads['dW' + str(l)]
        parameters['b' + str(l)] -= alpha*grads['db' + str(l)]
    return parameters


#########################################
###### Train and predict functions ######
#########################################


#  3.a  parameters, costs
def L_layer_model(X, Y, layers_dims, learning_rate, num_iterations, batch_size, use_batchnorm, use_dropout, keep_prob):
    """
        Description:
        Implements a L-layer neural network. All layers but the last should have the ReLU activation function,
        and the final layer will apply the softmax activation function.
        The size of the output layer should be equal to the number of labels in the data.
        Please select a batch size that enables your code to run well (i.e. no memory overflows while still running
        relatively fast).
        Hint: the function should use the earlier functions in the following order:
        initialize -> L_model_forward -> compute_cost -> L_model_backward -> update parameters
        Input:
        X – the input data, a numpy array of shape (height*width , number_of_examples)
        Comment: since the input is in grayscale we only have height and width,
                 otherwise it would have been height*width*3
        Y – the “real” labels of the data, a vector of shape (num_of_classes, number of examples)
        Layer_dims – a list containing the dimensions of each layer, including the input
        batch_size – the number of examples in a single training batch.
        Output:
        parameters – the parameters learnt by the system during the training
        (the same parameters that were updated in the update_parameters function).
        costs – the values of the cost function (calculated by the compute_cost function).
        One value is to be saved after each 100 training iterations (e.g. 3000 iterations -> 30 values).
    """
    test_size = 0.2  # proportion of validation out of all train samples
    X_train, X_validation, y_train, y_validation = train_test_split(X.T, Y.T, test_size=test_size, shuffle=True)
    # (48000, 784), (12000, 784), (48000, 10), (12000, 10)

    m = X_train.shape[0]  # m = 48000
    parameters = initialize_parameters(layers_dims)
    epsilon = sys.float_info.epsilon
    costs = {}
    prev_v_acc = 0
    curr_v_acc = 0
    # epoch_iteration = m // batch_size
    b = 1
    while b < (num_iterations + 1):
        # print("Batch #" + str(b) + "...")
        X_batch = X_train[(b - 1)*batch_size % m:b*batch_size % (m + 1), :]
        y_batch = y_train[(b - 1)*batch_size % m:b*batch_size % (m + 1), :]
        # if use_dropout and (b % epoch_iteration) == 0:
        #     parameters = apply_dropout(parameters, keep_prob)
        AL, caches = L_model_forward(X_batch.T, parameters, use_batchnorm, use_dropout, keep_prob)
        cost = compute_cost(AL, y_batch.T)
        grads = L_model_backward(AL, y_batch.T, caches)
        parameters = Update_parameters(parameters, grads, learning_rate)
        if (b % 100) == 0:
            epochs = b * batch_size // m
            costs['Epoch #' + str(epochs) + ', Batch #' + str(b)] = cost
            curr_v_acc = Predict(X_validation.T, y_validation.T, parameters, use_batchnorm)
            # print("Batch #" + str(b) + "..." + "curr_acc " + str(curr_v_acc))
            if abs(curr_v_acc - prev_v_acc) < epsilon:
                b = (num_iterations + 1)
            prev_v_acc = curr_v_acc
            # print("\tprev_acc " + str(prev_v_acc))
        b += 1
    curr_t_acc = Predict(X_train.T, y_train.T, parameters, use_batchnorm)
    curr_t_acc = float("{:.3f}".format(curr_t_acc))
    print("Train Accuracy = " + str(curr_t_acc) + "%")
    curr_v_acc = float("{:.3f}".format(curr_v_acc))
    print("Validation Accuracy = " + str(curr_v_acc) + "%")
    return parameters, costs


#  Auxiliary function
def to_one_hot(v, num_of_classes):
    temp = np.zeros((num_of_classes, v.size))
    temp[v, np.arange(v.size)] = 1
    return temp


#  3.b  accuracy
def Predict(X, Y, parameters, use_batchnorm):
    """
        Description:
        The function receives an input data and the true labels
        and calculates the accuracy of the trained neural network on the data.
        Input:
        X – the input data, a numpy array of shape (height*width, number_of_examples)
        Y – the “real” labels of the data, a vector of shape (num_of_classes, number of examples)
        Parameters – a python dictionary containing the DNN architecture’s parameters
        Output:
        accuracy – the accuracy measure of the neural net on the provided data
        (i.e. the percentage of the samples for which the correct label receives the highest confidence score).
        Use the softmax function to normalize the output values.
    """
    num_of_classes = 10  # digits
    m = X.shape[1]  # no. of samples
    AL, _ = L_model_forward(X, parameters, use_batchnorm, False, [])
    AL_indices_vector = np.argmax(AL, axis=0)
    AL_one_hot_matrix = to_one_hot(AL_indices_vector, num_of_classes)
    accuracy_matrix = AL_one_hot_matrix * Y
    accuracy = (np.sum(accuracy_matrix) / m) * 100
    return accuracy


def print_dict_to_csv(dict, csv_title):
    with open(csv_title, 'w') as f:
        for key in dict.keys():
            f.write("%s, %s\n" % (key, dict[key]))
    return


# keep_prob = [1, 0.75, 0.857, 0.8, 1]
# def apply_dropout(parameters, keep_prob):
#     L = len(parameters) // 2
#     n = parameters['W' + str(1)].shape[0]
#     removed_nodes = np.array([range(1, n + 1)])
#     mask = np.random.rand(1, n) > keep_prob[1]
#     removed_nodes *= mask
#     removed_nodes = removed_nodes[removed_nodes != 0]
#     m = removed_nodes.shape[0]
#     for i in range(m):
#         parameters['W' + str(1)][removed_nodes[i] - 1] = 0
#     for hl in range(2, L):
#         prev_removed_nodes = removed_nodes
#         for i in range(m):
#             parameters['W' + str(hl)][:, prev_removed_nodes[i] - 1] = 0
#         n = parameters['W' + str(hl)].shape[0]
#         removed_nodes = np.array([range(1, n + 1)])
#         mask = np.random.rand(1, n) > keep_prob[hl]
#         removed_nodes *= mask
#         removed_nodes = removed_nodes[removed_nodes != 0]
#         m = removed_nodes.shape[0]
#         for i in range(m):
#             parameters['W' + str(hl)][removed_nodes[i] - 1] = 0
#     return parameters


######################################################
######################## MAIN ########################
######################################################


(X, y), (X_test, y_test) = mnist.load_data()
m_X = X.shape[0]  # Total no. of X samples/images, 60000
m_X_test = X_test.shape[0]  # Total no. X_test of samples/images, 10000
n = X[0].shape[0]*X[0].shape[1]  # Size of "flattened" image, no. of features (= 28*28=784)

X = X.reshape(m_X, n) / 255  # (60000, 784)
X_test = X_test.reshape(m_X_test, n) / 255  # (10000, 784)
num_of_classes = 10  # digits
y = to_one_hot(y, num_of_classes)  # (10, 60000)
y_test = to_one_hot(y_test, num_of_classes) # (10, 60000)

alpha = 0.009
layer_dims = [n, 20, 7, 5, num_of_classes]
keep_prob = [1, 0.8, 1, 1, 1]
##############################################################
num_iterations = 32000  # no. of batches  # 8*1000*4
batch_size = 48  # 6*8
##############################################################

print("========== Dropout OFF ==========")
print()
print("==== Batchnorm OFF ====")
start_time = time.time()
parameters1, costs1 = L_layer_model(X=X.T, Y=y, layers_dims=layer_dims,
                                    learning_rate=alpha, num_iterations=num_iterations,
                                    batch_size=batch_size, use_batchnorm=False, use_dropout=False,
                                    keep_prob=keep_prob)
accuracy1 = Predict(X=X_test.T, Y=y_test, parameters=parameters1, use_batchnorm=False)
run_time = time.time() - start_time
accuracy1 = float("{:.3f}".format(accuracy1))
print("Test Accuracy = " + str(accuracy1) + "%")
print_dict_to_csv(costs1, "batchnorm_OFF_dropout_OFF.csv")
print("----------------------")
print("\t Running Time: %s [s]" % run_time)

# time.sleep(20)
print()
print("==== Batchnorm ON ====")
start_time = time.time()
parameters2, costs2 = L_layer_model(X=X.T, Y=y, layers_dims=layer_dims,
                                    learning_rate=alpha, num_iterations=num_iterations,
                                    batch_size=batch_size, use_batchnorm=True, use_dropout=False,
                                    keep_prob=keep_prob)
accuracy2 = Predict(X=X_test.T, Y=y_test, parameters=parameters2, use_batchnorm=True)
run_time = time.time() - start_time
accuracy2 = float("{:.3f}".format(accuracy2))
print("Test Accuracy = " + str(accuracy2) + "%")
print_dict_to_csv(costs2, "batchnorm_ON_dropout_OFF.csv")
print("----------------------")
print("\t Running Time: %s [s]" % run_time)

# time.sleep(20)
print()
print()
print("========== Dropout ON ==========")
print()
print("==== Batchnorm OFF ====")
start_time = time.time()
parameters3, costs3 = L_layer_model(X=X.T, Y=y, layers_dims=layer_dims,
                                    learning_rate=alpha, num_iterations=num_iterations,
                                    batch_size=batch_size, use_batchnorm=False, use_dropout=True,
                                    keep_prob=keep_prob)
accuracy3 = Predict(X=X_test.T, Y=y_test, parameters=parameters3, use_batchnorm=False)
run_time = time.time() - start_time
accuracy3 = float("{:.3f}".format(accuracy3))
print("Test Accuracy = " + str(accuracy3) + "%")
print_dict_to_csv(costs3, "batchnorm_OFF_dropout_ON.csv")
print("----------------------")
print("\t Running Time: %s [s]" % run_time)

# time.sleep(20)
print()
print("==== Batchnorm ON ====")
start_time = time.time()
parameters4, costs4 = L_layer_model(X=X.T, Y=y, layers_dims=layer_dims,
                                    learning_rate=alpha, num_iterations=num_iterations,
                                    batch_size=batch_size, use_batchnorm=True, use_dropout=True,
                                    keep_prob=keep_prob)
accuracy4 = Predict(X=X_test.T, Y=y_test, parameters=parameters4, use_batchnorm=True)
run_time = time.time() - start_time
accuracy4 = float("{:.3f}".format(accuracy4))
print("Test Accuracy = " + str(accuracy4) + "%")
print_dict_to_csv(costs4, "batchnorm_ON_dropout_ON.csv")
print("----------------------")
print("\t Running Time: %s [s]" % run_time)
